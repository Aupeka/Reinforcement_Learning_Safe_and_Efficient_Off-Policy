# Reinforcement Learning Safe and Efficient Off-Policy
In this report, we review the paper titled ”Safe and Efficient Off-Policy Reinforcement Learning”, authored by Remi Munos, Thomas Stepleton, Anna Harutyunyan, and Marc G. Bellemare. This article discusses the development of a new algorithm, Retrace(λ), for off-policy reinforcement learning, which addresses the trade-off between return-based and value bootstrap methods, and demonstrates low variance, safe and efficient learning from off-policy data, and convergence guarantees for both policy evaluation and control settings. After analyzing the theoritical results of the paper and their proofs, we proceeded to implement the four distinct algorithms under comparison within the article and compare them using two different GymAI environments, Cliff Walking V0 and Frozen Lake V1, for our simulation
